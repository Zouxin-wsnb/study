Artificial Intelligence (AI)
 Definition: Technologies and methods that enable machines to simulate(æ¨¡æ‹Ÿ) human-like intelligence, such that they carry out tasks in a way that we would consider "smart" .
 Typical AI tasks: reasoning, problem-solving, understanding natural language, and perception.
Machine Learning (ML)
 Definition: A series of algorithms and statistical models that enable computers to learn from experience (data), such that they make predictions or decisions without being explicitly(æ˜æ˜¾çš„) programmed.
 Typical ML tasks: regression(å›å½’), classification, clustering

Linear Regression(çº¿æ€§å›å½’):
 ğ‘“(ğ’™)=ğ’˜â‹…ğ’™=[ğ‘¤_1,â€¦,ğ‘¤_ğ‘‘ ][â–ˆ(ğ‘¥_1@â€¦@ğ‘¥_ğ‘‘ )]=ğ‘¤_1 ğ‘¥_1+â€¦+ğ‘¤_ğ‘‘ ğ‘¥_ğ‘‘
 ğ’™âˆˆâ„^ğ‘‘: Features of Example  
 ğ’˜âˆˆâ„^ğ‘‘: Parameters
 Optimize by *Gradient Decent*:
  Gradient âˆ‡_ğ’˜ MSELoss(ğ’˜)  is the direction that increases the loss the most

Logistic Regression:
 *Logistic regression is used for Binary Classification
 Penalty/Regularization: 
  Control the model complexity
  Avoid overfitting on training data
  *Smaller C constrains the model more
  *L1 leads to sparser(ç¨€) model weights

K-Nearest Neighbors (KNN):
 1.Find K nearest neighbors (according to     some distance metrics)
 2.Assign the majority label to the test point
 KNN is a non-liner classifier

Evaluation Metrics:
 1.Confusion Matrix:(T:True(é¢„æµ‹å¯¹äº†çš„);P:Positive(æ­£ç±»ï¼Œé˜³);N:Negative(è´Ÿç±»ï¼Œé˜´))
  Accuracy = (ğ‘‡ğ‘ƒ+ğ‘‡ğ‘)/ğ‘›
  Recall = ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘)
  Precision = ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ)
  F1 Score=(2â‹…Precisionâ‹…Recall)/(Precision+Recall)(å¦‚æœF1åˆ†æ•°è¾ƒé«˜ï¼Œè¯´æ˜æ¨¡å‹åœ¨é¢„æµ‹æ­£ç±»æ—¶è¡¨ç°è‰¯å¥½)
  ROC(Receiver Operating Characteristic) Curve
   ROC-AUC Score: Area Under the ROC curve(è¶Šé è¿‘å·¦ä¸Šè¶Šå¥½)
  
Parameter Tuning and Selection
 '''
 è®­ç»ƒæ•°æ® (Training Data)ï¼š
  ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹é€šè¿‡è®­ç»ƒæ•°æ®å­¦ä¹ åˆ°å‚æ•°ã€‚
 éªŒè¯æ•°æ® (Validation Data)ï¼š
  ç”¨äºè°ƒä¼˜å’Œé€‰æ‹©æ¨¡å‹çš„è¶…å‚æ•°ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ï¼Œä½†ä¸ç”¨äºè®­ç»ƒæ¨¡å‹æœ¬èº«ã€‚éªŒè¯æ•°æ®é›†æ˜¯ç”¨æ¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ¯”ä¸åŒæ¨¡å‹æˆ–è¶…å‚æ•°è®¾ç½®çš„æ€§èƒ½çš„ã€‚
 æµ‹è¯•æ•°æ® (Test Data)ï¼š
  ç”¨äºæœ€ç»ˆè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œé€šå¸¸åœ¨æ¨¡å‹å®Œå…¨è®­ç»ƒå¥½ä¹‹åä½¿ç”¨ã€‚æµ‹è¯•æ•°æ®åº”å½“ä¸è®­ç»ƒå’ŒéªŒè¯æ•°æ®ç›¸äº’ç‹¬ç«‹ï¼Œç”¨äºæ£€éªŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
 '''
 K-fold Cross Validation:
  Tune model based on average performance 
  Avoid Overfitting on a selected validation set
 Tuning Hyper-parameter with CV:
  eg:scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracyâ€™)
  è¿™é‡Œcvæ˜¯äº¤å‰éªŒè¯çš„æŠ˜æ•°

Decision Tree:
 Some Hyper-parameters:
  max_depth: float, default=None. The maximum depth of the tree
  min_samples_leaf: The minimum number of samples required to be at a leaf node

Random Forest:
 
XGBoost:

--------------------------------
Neural Networks(ç¥ç»ç½‘ç»œ): 
 Multi-Layer Perception:MLP is composed of multiple layers of neural networks
  Optimization of MLP:
   Learn model parameters to minimize loss functions
   Optimize by gradient descent with back propagation
   é€šè¿‡æ¢¯åº¦ä¸‹é™å’Œåå‘ä¼ æ’­è¿›è¡Œä¼˜åŒ–
   Optimization is sensitive to the weight initialization
 Universal Approximation Theorem(é€¼è¿‘å®šç†):
  Regardless of the activation function, the dimension of the input space, a neural network with one hidden layer can approximate any continuous function
  æ— è®ºæ¿€æ´»å‡½æ•°æ˜¯ä»€ä¹ˆï¼Œæ— è®ºè¾“å…¥ç©ºé—´çš„ç»´åº¦æ˜¯å¤šå°‘ï¼Œä¸€ä¸ªåªæœ‰ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œéƒ½å¯ä»¥è¿‘ä¼¼ä»»ä½•è¿ç»­å‡½æ•°
 Activation Functions:
  Sigmoid:ğ‘“(ğ‘¥)=1/(1+expâ¡(âˆ’ğ‘¥)), Mostly used for binary classification, Not often used in hidden layers now.
  ReLU:ğ‘“(ğ‘¥)=maxâ¡(0,ğ‘¥), Good default choice for hidden layers
 

