{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229d5a38",
   "metadata": {},
   "source": [
    "## Pre-Task: Environment Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c459acf",
   "metadata": {},
   "source": [
    "## Iris Classification\n",
    "We will explore some foundational machine learning techniques focused on classification tasks. Through this notebook, you will gain hands-on experience in building, evaluating, and optimizing models that can make predictions from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62670ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a975f7",
   "metadata": {},
   "source": [
    "### Task 1. Complete the code below to split the dataset into training and test sets.\n",
    "Please follow the instructions following:\n",
    "1. Use the train_test_split function from the sklearn.model_selection module to perform the split. \n",
    "2. Allocate 80% of the data to the training set and 20% to the test set. \n",
    "3. Use a random state of 42 to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44541c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (569, 30)\n",
      "Sample data point: [1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      " 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      " 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      " 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      " 4.601e-01 1.189e-01]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = # please split data into training and test sets\n",
    "\n",
    "# View some data points\n",
    "print(\"Data shape:\", X.shape)\n",
    "print(\"Sample data point:\", X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a7d31",
   "metadata": {},
   "source": [
    "\n",
    "We will delve into three popular machine learning models:\n",
    "\n",
    "1. Decision Tree Classifier: A straightforward yet powerful model that represents decisions and decision-making in a tree-like graph of choices.\n",
    "2. Random Forest Classifier: An ensemble model that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes of the individual trees. It is known for its high accuracy, robustness, and ease of use.\n",
    "3. Gradient Boosting Classifier (XGBoost): Another ensemble technique that builds models sequentially, with each new model attempting to correct errors made by the previous ones. It is particularly powerful and flexible, widely used in many winning Kaggle competition entries.\n",
    "\n",
    "The decision tree model and random forest model are created in the following as examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e46b372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9473684210526315\n",
      "Random Forest Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train a decision tree classifier\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "tree_predictions = tree_model.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, tree_predictions))\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481590b2",
   "metadata": {},
   "source": [
    "### Task 2: Building Gradient Boosting models\n",
    "**Please bulid a gradient boosting model with sklearn**\n",
    "1. Use the GradientBoostingClassifier and set n_estimators=100 and random_state=100.\n",
    "2. Fit the model on the X_train and y_train datasets.\n",
    "3. Predict the test set results and calculate the accuracy using the accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453602f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and train the Gradient Boosting model on X_train and y_train.\n",
    "\n",
    "# Obtain the predictions from the trained model\n",
    "\n",
    "# Give the accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cac9a6",
   "metadata": {},
   "source": [
    "### Examples of Hyperparameter Tuning on Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49fedd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Decision Tree - Best Parameters: {'criterion': 'gini', 'max_depth': None}\n",
      "Decision Tree - Best cross-validation score: 0.93\n",
      "Tuned Decision Tree Accuracy: 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for Decision Tree\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30, 50]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object for Decision Tree\n",
    "grid_search_dt = GridSearchCV(estimator=DecisionTreeClassifier(random_state=100), \n",
    "                              param_grid=param_grid_dt, \n",
    "                              cv=3, verbose=1, scoring='accuracy')\n",
    "\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Decision Tree - Best Parameters:\", grid_search_dt.best_params_)\n",
    "print(\"Decision Tree - Best cross-validation score: {:.2f}\".format(grid_search_dt.best_score_))\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "final_predictions_dt = best_dt.predict(X_test)\n",
    "print(\"Tuned Decision Tree Accuracy:\", accuracy_score(y_test, final_predictions_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1252ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Random Forest - Best Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 50}\n",
      "Random Forest - Best cross-validation score: 0.96\n",
      "Tuned Random Forest Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [10, 20, 30],\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object for Random Forest\n",
    "grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=100), \n",
    "                              param_grid=param_grid_rf, \n",
    "                              cv=3, verbose=1, scoring='accuracy')\n",
    "\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Random Forest - Best Parameters:\", grid_search_rf.best_params_)\n",
    "print(\"Random Forest - Best cross-validation score: {:.2f}\".format(grid_search_rf.best_score_))\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "final_predictions_rf = best_rf.predict(X_test)\n",
    "print(\"Tuned Random Forest Accuracy:\", accuracy_score(y_test, final_predictions_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e78b50",
   "metadata": {},
   "source": [
    "### Task 3. Hyper-parameter tuning on gradient boosting.\n",
    "\n",
    "Define the parameter grid for gradient boosting, specifying the range of values to explore for each parameter. You are required to vary:\n",
    "1. **Number of Estimators (n_estimators)**: Explore values of 10, 50, 100, and 200 to determine the optimal number of trees in the model.\n",
    "2. **Learning Rate (learning_rate)**: Test learning rates of 0.01, 0.1, and 0.2 to control the contribution of each tree.\n",
    "3. **Maximum Depth (max_depth)**: Vary the tree depth with values of 3, 5, and 10 to find the best balance between model complexity and overfitting.\n",
    "\n",
    "Also, set up a GridSearchCV object using this parameter grid to automate the process of finding the best model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Gradient Boosting\n",
    "param_grid_gb = \n",
    "\n",
    "# Create a GridSearchCV object for Gradient Boosting\n",
    "grid_search_gb = \n",
    "\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Gradient Boosting - Best Parameters:\", grid_search_gb.best_params_)\n",
    "print(\"Gradient Boosting - Best cross-validation score: {:.2f}\".format(grid_search_gb.best_score_))\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_gb = grid_search_gb.best_estimator_\n",
    "final_predictions_gb = best_gb.predict(X_test)\n",
    "print(\"Tuned Gradient Boosting Accuracy:\", accuracy_score(y_test, final_predictions_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38ff9d",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP) on the Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a56ab232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAGkCAYAAAAIduO+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYoElEQVR4nO3df2yUhR3H8c/R2oNpexak0I7jpygCtoMWCKvOHyCkQSL7oxKCWYXNRXJMsDFx/WewLOPqH1twGyk/xoqJYyDLis4MusKkZJkdpaQJaIJgmRwidG5wV7rkML3bX7utQ9o+R788PNf3K3midz7HfUIqb+5He75kMpkUAABGhrk9AACQ2QgNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAVMaEZsuWLZo4caKGDx+uefPm6dixY25P6tfRo0e1dOlSFRUVyefzaf/+/W5PGpBwOKw5c+YoNzdXBQUFWrZsmU6fPu32rAGpq6tTcXGx8vLylJeXp/nz5+vAgQNuz3KstrZWPp9P69evd3tKvzZu3Cifz9frmDZtmtuzBuTTTz/Vc889p1GjRmnEiBF6+OGHdfz4cbdn9WvixIk3/J77fD6FQiFX9mREaPbu3avq6mpt2LBBJ06cUElJiRYvXqzOzk63p/Wpu7tbJSUl2rJli9tTHGlublYoFFJLS4uampr0xRdfaNGiReru7nZ7Wr/GjRun2tpatbW16fjx43ryySf1zDPP6IMPPnB72oC1trZq27ZtKi4udnvKgM2YMUOfffZZ6vjzn//s9qR+XblyReXl5brrrrt04MABffjhh/rJT36i/Px8t6f1q7W1tdfvd1NTkySpsrLSnUHJDDB37txkKBRKXe7p6UkWFRUlw+Gwi6uckZRsaGhwe0ZaOjs7k5KSzc3Nbk9JS35+fvKXv/yl2zMGpKurKzl16tRkU1NT8rHHHkuuW7fO7Un92rBhQ7KkpMTtGY69+uqryUceecTtGYNi3bp1ySlTpiQTiYQr9+/5RzTXr19XW1ubFi5cmLpu2LBhWrhwod5//30Xlw0d0WhUkjRy5EiXlzjT09OjPXv2qLu7W/Pnz3d7zoCEQiEtWbKk19e7F5w5c0ZFRUWaPHmyVq5cqfPnz7s9qV/vvPOOysrKVFlZqYKCAs2aNUs7duxwe5Zj169f15tvvqnVq1fL5/O5ssHzofn888/V09OjMWPG9Lp+zJgxunTpkkurho5EIqH169ervLxcM2fOdHvOgJw8eVL33HOP/H6/XnzxRTU0NGj69Oluz+rXnj17dOLECYXDYbenODJv3jzt2rVLBw8eVF1dnc6dO6dHH31UXV1dbk/rU0dHh+rq6jR16lQ1NjZqzZo1eumll/TGG2+4Pc2R/fv36+rVq3r++edd25Dt2j0jI4RCIZ06dcoTz7n/x4MPPqj29nZFo1H99re/VVVVlZqbm+/o2EQiEa1bt05NTU0aPny423McqaioSP17cXGx5s2bpwkTJuitt97St7/9bReX9S2RSKisrEybNm2SJM2aNUunTp3S1q1bVVVV5fK6gdu5c6cqKipUVFTk2gbPP6K57777lJWVpcuXL/e6/vLlyxo7dqxLq4aGtWvX6t1339V7772ncePGuT1nwHJycnT//fertLRU4XBYJSUlev31192e1ae2tjZ1dnZq9uzZys7OVnZ2tpqbm/Wzn/1M2dnZ6unpcXvigN1777164IEHdPbsWben9KmwsPCGv3w89NBDnnja7z8++eQTHTp0SN/5zndc3eH50OTk5Ki0tFSHDx9OXZdIJHT48GHPPO/uNclkUmvXrlVDQ4P+9Kc/adKkSW5PuiWJRELxeNztGX1asGCBTp48qfb29tRRVlamlStXqr29XVlZWW5PHLBr167p448/VmFhodtT+lReXn7D2/Y/+ugjTZgwwaVFztXX16ugoEBLlixxdUdGPHVWXV2tqqoqlZWVae7cudq8ebO6u7u1atUqt6f16dq1a73+Vnfu3Dm1t7dr5MiRGj9+vIvL+hYKhbR79269/fbbys3NTb0WFggENGLECJfX9a2mpkYVFRUaP368urq6tHv3bh05ckSNjY1uT+tTbm7uDa+B3X333Ro1atQd/9rYK6+8oqVLl2rChAm6ePGiNmzYoKysLK1YscLtaX16+eWX9fWvf12bNm3Ss88+q2PHjmn79u3avn2729MGJJFIqL6+XlVVVcrOdvmPelfe62bg5z//eXL8+PHJnJyc5Ny5c5MtLS1uT+rXe++9l5R0w1FVVeX2tD592WZJyfr6eren9Wv16tXJCRMmJHNycpKjR49OLliwIPnHP/7R7Vlp8crbm5cvX54sLCxM5uTkJL/61a8mly9fnjx79qzbswbk97//fXLmzJlJv9+fnDZtWnL79u1uTxqwxsbGpKTk6dOn3Z6S9CWTyaQ7iQMADAWef40GAHBnIzQAAFOEBgBgitAAAEwRGgCAKUIDADCVUaGJx+PauHHjHf9d3v/Pq7sl72736m7Ju9u9ulvy7vY7ZXdGfR9NLBZTIBBQNBpVXl6e23MGzKu7Je9u9+puybvbvbpb8u72O2V3Rj2iAQDceQgNAMDUbf9Ja4lEQhcvXlRubu6gf9pbLBbr9U+v8Opuybvbvbpb8u52r+6WvLvdencymVRXV5eKioo0bNjNH7fc9tdoLly4oGAweDvvEgBgKBKJ9PmZVLf9EU1ubu7tvktIWrZsmdsT0rJx40a3J6TtyJEjbk9Ii5d/z69ever2hCGpvz/Xb3toBvvpMgzMXXfd5faEtHj5LyZ3+mfz3Az/j8Kp/r5meDMAAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACm0grNli1bNHHiRA0fPlzz5s3TsWPHBnsXACBDOA7N3r17VV1drQ0bNujEiRMqKSnR4sWL1dnZabEPAOBxjkPz05/+VC+88IJWrVql6dOna+vWrfrKV76iX/3qVxb7AAAe5yg0169fV1tbmxYuXPjfX2DYMC1cuFDvv//+l94mHo8rFov1OgAAQ4ej0Hz++efq6enRmDFjel0/ZswYXbp06UtvEw6HFQgEUkcwGEx/LQDAc8zfdVZTU6NoNJo6IpGI9V0CAO4g2U5Ovu+++5SVlaXLly/3uv7y5csaO3bsl97G7/fL7/envxAA4GmOHtHk5OSotLRUhw8fTl2XSCR0+PBhzZ8/f9DHAQC8z9EjGkmqrq5WVVWVysrKNHfuXG3evFnd3d1atWqVxT4AgMc5Ds3y5cv197//XT/4wQ906dIlfe1rX9PBgwdveIMAAABSGqGRpLVr12rt2rWDvQUAkIH4WWcAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJhK64PP4D21tbVuT0jL5MmT3Z6Qtvz8fLcnpOWf//yn2xPS9uyzz7o9IS379u1ze4IpHtEAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMOU4NEePHtXSpUtVVFQkn8+n/fv3G8wCAGQKx6Hp7u5WSUmJtmzZYrEHAJBhsp3eoKKiQhUVFRZbAAAZyHFonIrH44rH46nLsVjM+i4BAHcQ8zcDhMNhBQKB1BEMBq3vEgBwBzEPTU1NjaLRaOqIRCLWdwkAuIOYP3Xm9/vl9/ut7wYAcIfi+2gAAKYcP6K5du2azp49m7p87tw5tbe3a+TIkRo/fvygjgMAeJ/j0Bw/flxPPPFE6nJ1dbUkqaqqSrt27Rq0YQCAzOA4NI8//riSyaTFFgBABuI1GgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATDn+4LOhrLS01O0JaZs8ebLbE9IyZcoUtyekraOjw+0JaWlqanJ7Qtq8+v/ovn373J5gikc0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgylFowuGw5syZo9zcXBUUFGjZsmU6ffq01TYAQAZwFJrm5maFQiG1tLSoqalJX3zxhRYtWqTu7m6rfQAAj8t2cvLBgwd7Xd61a5cKCgrU1tamb3zjG4M6DACQGRyF5v9Fo1FJ0siRI296TjweVzweT12OxWK3cpcAAI9J+80AiURC69evV3l5uWbOnHnT88LhsAKBQOoIBoPp3iUAwIPSDk0oFNKpU6e0Z8+ePs+rqalRNBpNHZFIJN27BAB4UFpPna1du1bvvvuujh49qnHjxvV5rt/vl9/vT2scAMD7HIUmmUzqe9/7nhoaGnTkyBFNmjTJahcAIEM4Ck0oFNLu3bv19ttvKzc3V5cuXZIkBQIBjRgxwmQgAMDbHL1GU1dXp2g0qscff1yFhYWpY+/evVb7AAAe5/ipMwAAnOBnnQEATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYMrRB58Ndfn5+W5PSFtbW5vbE9LS0dHh9oQhx6tfK7hz8YgGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClHoamrq1NxcbHy8vKUl5en+fPn68CBA1bbAAAZwFFoxo0bp9raWrW1ten48eN68skn9cwzz+iDDz6w2gcA8LhsJycvXbq01+Uf//jHqqurU0tLi2bMmDGowwAAmcFRaP5XT0+P9u3bp+7ubs2fP/+m58XjccXj8dTlWCyW7l0CADzI8ZsBTp48qXvuuUd+v18vvviiGhoaNH369JueHw6HFQgEUkcwGLylwQAAb3EcmgcffFDt7e3661//qjVr1qiqqkoffvjhTc+vqalRNBpNHZFI5JYGAwC8xfFTZzk5Obr//vslSaWlpWptbdXrr7+ubdu2fen5fr9ffr//1lYCADzrlr+PJpFI9HoNBgCA/+XoEU1NTY0qKio0fvx4dXV1affu3Tpy5IgaGxut9gEAPM5RaDo7O/Wtb31Ln332mQKBgIqLi9XY2KinnnrKah8AwOMchWbnzp1WOwAAGYqfdQYAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClHH3w21OXn57s9IW2HDh1yewI8wstf51euXHF7Ar4Ej2gAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMDULYWmtrZWPp9P69evH6Q5AIBMk3ZoWltbtW3bNhUXFw/mHgBAhkkrNNeuXdPKlSu1Y8cO5efnD/YmAEAGSSs0oVBIS5Ys0cKFC/s9Nx6PKxaL9ToAAENHttMb7NmzRydOnFBra+uAzg+Hw/rhD3/oeBgAIDM4ekQTiUS0bt06/frXv9bw4cMHdJuamhpFo9HUEYlE0hoKAPAmR49o2tra1NnZqdmzZ6eu6+np0dGjR/WLX/xC8XhcWVlZvW7j9/vl9/sHZy0AwHMchWbBggU6efJkr+tWrVqladOm6dVXX70hMgAAOApNbm6uZs6c2eu6u+++W6NGjbrhegAAJH4yAADAmON3nf2/I0eODMIMAECm4hENAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmbvmDz4aSK1euuD0hbaWlpW5PGHLy8/PdnpAWL3+t7Nu3z+0J+BI8ogEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgylFoNm7cKJ/P1+uYNm2a1TYAQAbIdnqDGTNm6NChQ//9BbId/xIAgCHEcSWys7M1duxYiy0AgAzk+DWaM2fOqKioSJMnT9bKlSt1/vz5Ps+Px+OKxWK9DgDA0OEoNPPmzdOuXbt08OBB1dXV6dy5c3r00UfV1dV109uEw2EFAoHUEQwGb3k0AMA7HIWmoqJClZWVKi4u1uLFi/WHP/xBV69e1VtvvXXT29TU1CgajaaOSCRyy6MBAN5xS6/k33vvvXrggQd09uzZm57j9/vl9/tv5W4AAB52S99Hc+3aNX388ccqLCwcrD0AgAzjKDSvvPKKmpub9be//U1/+ctf9M1vflNZWVlasWKF1T4AgMc5eurswoULWrFihf7xj39o9OjReuSRR9TS0qLRo0db7QMAeJyj0OzZs8dqBwAgQ/GzzgAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMOXog8+Guo6ODrcnpK20tNTtCWmprKx0e0LavLzdq1577TW3J+BL8IgGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMOQ7Np59+queee06jRo3SiBEj9PDDD+v48eMW2wAAGSDbyclXrlxReXm5nnjiCR04cECjR4/WmTNnlJ+fb7UPAOBxjkLz2muvKRgMqr6+PnXdpEmTBn0UACBzOHrq7J133lFZWZkqKytVUFCgWbNmaceOHX3eJh6PKxaL9ToAAEOHo9B0dHSorq5OU6dOVWNjo9asWaOXXnpJb7zxxk1vEw6HFQgEUkcwGLzl0QAA73AUmkQiodmzZ2vTpk2aNWuWvvvd7+qFF17Q1q1bb3qbmpoaRaPR1BGJRG55NADAOxyFprCwUNOnT+913UMPPaTz58/f9DZ+v195eXm9DgDA0OEoNOXl5Tp9+nSv6z766CNNmDBhUEcBADKHo9C8/PLLamlp0aZNm3T27Fnt3r1b27dvVygUstoHAPA4R6GZM2eOGhoa9Jvf/EYzZ87Uj370I23evFkrV6602gcA8DhH30cjSU8//bSefvppiy0AgAzEzzoDAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMCU4w8+G8o6OjrcnpC273//+25PSEttba3bE9LW1tbm9oS0lJWVuT0BGYZHNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMOQrNxIkT5fP5bjhCoZDVPgCAx2U7Obm1tVU9PT2py6dOndJTTz2lysrKQR8GAMgMjkIzevToXpdra2s1ZcoUPfbYY4M6CgCQORyF5n9dv35db775pqqrq+Xz+W56XjweVzweT12OxWLp3iUAwIPSfjPA/v37dfXqVT3//PN9nhcOhxUIBFJHMBhM9y4BAB6Udmh27typiooKFRUV9XleTU2NotFo6ohEIuneJQDAg9J66uyTTz7RoUOH9Lvf/a7fc/1+v/x+fzp3AwDIAGk9oqmvr1dBQYGWLFky2HsAABnGcWgSiYTq6+tVVVWl7Oy030sAABgiHIfm0KFDOn/+vFavXm2xBwCQYRw/JFm0aJGSyaTFFgBABuJnnQEATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABTt/0jMvksG3dcv37d7Qlp6erqcntC2v71r3+5PQG4Lfr7c92XvM1/8l+4cEHBYPB23iUAwFAkEtG4ceNu+t9ve2gSiYQuXryo3Nxc+Xy+Qf21Y7GYgsGgIpGI8vLyBvXXtuTV3ZJ3t3t1t+Td7V7dLXl3u/XuZDKprq4uFRUVadiwm78Sc9ufOhs2bFif5RsMeXl5nvpi+A+v7pa8u92ruyXvbvfqbsm72y13BwKBfs/hzQAAAFOEBgBgKqNC4/f7tWHDBvn9frenOOLV3ZJ3t3t1t+Td7V7dLXl3+52y+7a/GQAAMLRk1CMaAMCdh9AAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABT/wYMQUBqKDC9pAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Visualize the first digit\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0]) \n",
    "plt.show()\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d948b76",
   "metadata": {},
   "source": [
    "### Build the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "715fa484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial MLP Accuracy: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MLPClassifier\n",
    "mlp_model = MLPClassifier(random_state=1, max_iter=300)\n",
    "\n",
    "# Fit the model on training data\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the initial model\n",
    "initial_predictions = mlp_model.predict(X_test)\n",
    "print(\"Initial MLP Accuracy:\", accuracy_score(y_test, initial_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d614d59",
   "metadata": {},
   "source": [
    "### Task 4. Visualization of the learning curve of MLP\n",
    "\n",
    "A learning curve is a line plot that shows the decrease of loss over training iterations. Specifically, in the line plot of learning curve:\n",
    "* Y-Axis (Loss): Represents loss of the model\n",
    "* X-Axis (Iterations): Shows the number of training cycles the model has completed.\n",
    "\n",
    "The loss curve data is stored in `mlp_model.loss_curve_`, where `mlp_model.loss_curve_[i]` represents the loss after training i iterations.\n",
    "**Please complete the code of ploting learning curve following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23abd715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "print(len(mlp_model.loss_curve_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9089405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "#Please complete to code of ploting learning curve.\n",
    "# TO Complete\n",
    "\n",
    "\n",
    "plt.title('Learning Curve of MLP')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
